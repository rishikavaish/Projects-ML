{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "building_classifier_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOtNE4_26SMH"
      },
      "source": [
        "# **BUILDING THE CLASSIFIER MODEL BY APPLYING NLP**\n",
        "## Steps\n",
        "*   Scrape reviews from a google playstore apps\n",
        "*   Text Preprocessing\n",
        "*   Text clustering\n",
        "*   Fine tune a pretrained XLNet model\n",
        "*   Generate Predictions\n",
        "\n",
        "# Scrape Google Playstore reviews using google-play-scraper and MongoDB\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsYXXode6QRl"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# for scraping app info and reviews from Google Play\n",
        "from google_play_scraper import app, Sort, reviews\n",
        "\n",
        "# for pretty printing data structures\n",
        "from pprint import pprint\n",
        "\n",
        "# for storing in MongoDB\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# for keeping track of timing\n",
        "import datetime as dt\n",
        "from tzlocal import get_localzone\n",
        "\n",
        "# for building in wait times\n",
        "import random\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvOgap-J7ZZE"
      },
      "source": [
        "# Set up Mongo client\n",
        "client = MongoClient(host='localhost', port=27017)\n",
        "\n",
        "# Database for project\n",
        "app_proj_db = client['app_proj_db']\n",
        "\n",
        "# Set up new collection within project db for app reviews\n",
        "review_collection = app_proj_db['review_collection']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNVvBhmq7hLd"
      },
      "source": [
        "# choosing some random app names along with their app ids\n",
        "app_names = ['SBI',\n",
        "'UNION',\n",
        "'CBOI',\n",
        "'BOI',\n",
        "'CORP',\n",
        "'IOB',\n",
        "'LVB',\n",
        "'KVB']\n",
        "app_ids =  [\n",
        "  'com.freedomrewardz',\n",
        "'com.unionrewardz',\n",
        "'com.centrewardz',\n",
        "'com.boistarrewardz',\n",
        "'com.corprewardz',\n",
        "'com.iobrewardz',\n",
        "'com.lvbrewardz',\n",
        "'com.kvbrewardz'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvDQg0Em7zSE"
      },
      "source": [
        "#Looping through app_ids and app_names to get reviews\n",
        "for app_name, app_id in zip(app_names, app_ids):\n",
        "    \n",
        "    # Get the starting time\n",
        "    start = dt.datetime.now(tz=get_localzone())\n",
        "    fmt= \"%m/%d/%y - %T %p\"    \n",
        "    print('-------------------------------------------------------------------------')    \n",
        "    print(f'{app_name} started at {start.strftime(fmt)}')\n",
        "    print()\n",
        "    \n",
        "    # Empty list for storing reviews\n",
        "    app_reviews = []\n",
        "    \n",
        "    # Number of reviews to scrape per batch\n",
        "    count = 200\n",
        "    \n",
        "    # To keep track of how many batches have been completed\n",
        "    batch_num = 0\n",
        "    \n",
        "    \n",
        "    # Retrieve reviews (and continuation_token) with reviews function\n",
        "    rvws, token = reviews(\n",
        "        app_id,           # found in app's url\n",
        "        lang='en',        # defaults to 'en'\n",
        "        country='us',     # defaults to 'us'\n",
        "        sort=Sort.NEWEST, # start with most recent\n",
        "        count=count       # batch size\n",
        "    )\n",
        "    \n",
        "    \n",
        "    # For each review get an app name and app id\n",
        "    for r in rvws:\n",
        "        r['app_name'] = app_name \n",
        "        r['app_id'] = app_id     \n",
        "     \n",
        "    \n",
        "    # Add the list of review dicts to overall list\n",
        "    app_reviews.extend(rvws)\n",
        "    \n",
        "    # Increase batch count by one\n",
        "    batch_num +=1 \n",
        "    print(f'Batch {batch_num} completed.')\n",
        "    \n",
        "    # Wait 1 to 5 seconds to start next batch\n",
        "    time.sleep(random.randint(1,5))\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Append review IDs\n",
        "    pre_review_ids = []\n",
        "    for rvw in app_reviews:\n",
        "        pre_review_ids.append(rvw['reviewId'])\n",
        "    \n",
        "    \n",
        "    # Loop through at most max number of batches\n",
        "    for batch in range(4999):\n",
        "        rvws, token = reviews( \n",
        "            app_id,\n",
        "            lang='en',\n",
        "            country='us',\n",
        "            sort=Sort.NEWEST,\n",
        "            count=count,\n",
        "            # using token obtained from previous batch\n",
        "            continuation_token=token\n",
        "        )\n",
        "        \n",
        "        # Append unique review IDs from current batch to new list\n",
        "        new_review_ids = []\n",
        "        for r in rvws:\n",
        "            new_review_ids.append(r['reviewId'])\n",
        "            \n",
        "            # And add keys for name and id to the review dict\n",
        "            r['app_name'] = app_name \n",
        "            r['app_id'] = app_id     \n",
        "     \n",
        "        # Add the list of review dicts to app_reviews list\n",
        "        app_reviews.extend(rvws)\n",
        "        \n",
        "        # Increase batch count by one\n",
        "        batch_num +=1\n",
        "        \n",
        "        # Break loop and stop scraping for current app if most recent batch\n",
        "        # did not add any unique reviews\n",
        "        all_review_ids = pre_review_ids + new_review_ids\n",
        "        if len(set(pre_review_ids)) == len(set(all_review_ids)):\n",
        "            print(f'No reviews left to scrape. Completed {batch_num} batches.\\n')\n",
        "            break\n",
        "        \n",
        "        # all_review_ids becomes pre_review_ids to check against \n",
        "        # for next batch\n",
        "        pre_review_ids = all_review_ids\n",
        "        \n",
        "        \n",
        "        # At every 100th batch\n",
        "        if batch_num%100==0:\n",
        "            \n",
        "            # print update on number of batches\n",
        "            print(f'Batch {batch_num} completed.')\n",
        "            \n",
        "            # insert reviews into collection\n",
        "            review_collection.insert_many(app_reviews)\n",
        "            \n",
        "            # print update about num reviews inserted\n",
        "            store_time = dt.datetime.now(tz=get_localzone())\n",
        "            print(f\"\"\"\n",
        "            Successfully inserted {len(app_reviews)} {app_name} \n",
        "            reviews into collection at {store_time.strftime(fmt)}.\\n\n",
        "            \"\"\")\n",
        "            \n",
        "            # empty our list for next round of 100 batches\n",
        "            app_reviews = []\n",
        "        \n",
        "        # Wait 1 to 5 seconds to start next batch\n",
        "        time.sleep(random.randint(1,5))\n",
        "      \n",
        "    \n",
        "    # Print update when max number of batches has been reached\n",
        "    # OR when last batch didn't add any unique reviews\n",
        "    print(f'Done scraping {app_name}.')\n",
        "    print(f'Scraped a total of {len(set(pre_review_ids))} unique reviews.\\n')\n",
        "    \n",
        "    \n",
        "    # Insert remaining reviews into collection\n",
        "    review_collection.insert_many(app_reviews)\n",
        "    \n",
        "    # Get end time\n",
        "    end = dt.datetime.now(tz=get_localzone())\n",
        "    \n",
        "    # Print ending output for app\n",
        "    print(f\"\"\"\n",
        "    Successfully inserted all {app_name} reviews into collection\n",
        "    at {end.strftime(fmt)}.\\n\n",
        "    \"\"\")\n",
        "    print(f'Time taken to scrape reviews for {app_name}: {end-start}')\n",
        "    print('-----------------------------------------------------------------------')\n",
        "    print('\\n')\n",
        "    \n",
        "    # Wait 1 to 5 seconds to start scraping next app\n",
        "    time.sleep(random.randint(1,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGU35YCu757t"
      },
      "source": [
        "# converting the results into dataframe\n",
        "app_reviews_df = pd.DataFrame(list(review_collection.find({})))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27z2COMJ8TNs"
      },
      "source": [
        "#splitting the 'at' column into date and time separately\n",
        "\n",
        "# make string version of original column, call it 'col'\n",
        "app_reviews_df['col'] = app_reviews_df['at'].astype(str)\n",
        "\n",
        "# make the new columns using string indexing\n",
        "app_reviews_df['date'] = app_reviews_df['col'].str[0:11]\n",
        "app_reviews_df['time'] = app_reviews_df['col'].str[11:20]\n",
        "\n",
        "# get rid of the extra variable\n",
        "app_reviews_df.drop('col', axis=1, inplace=True)\n",
        "app_reviews_df.drop('at', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAkxRiOq8Xd9"
      },
      "source": [
        "app_reviews_df['date'] = pd.to_datetime(app_reviews_df['date'])\n",
        "  \n",
        "start_date = '01-01-2019'\n",
        "end_date = '03-31-2021'\n",
        "\n",
        "mask = (app_reviews_df['date'] > start_date) & (app_reviews_df['date'] <= end_date)\n",
        "app_reviews_df = app_reviews_df.loc[mask]\n",
        "\n",
        "del app_reviews_df['repliedAt']\n",
        "\n",
        "app_reviews_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdYBgMK48cHW"
      },
      "source": [
        "# you might get some duplicates. To remove duplicates:\n",
        "app_reviews_df = app_reviews_df.drop_duplicates(subset=['reviewId'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIRhntO28sBV"
      },
      "source": [
        "#convert the final dataframe into csv\n",
        "app_reviews_df.to_csv('reviews_final.csv', index=None, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZkIyB56eIYh"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmiXvh76eH-d"
      },
      "source": [
        "# Load Required Libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from textblob import TextBlob\n",
        "from textblob import Word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Cq4neplX0a"
      },
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('reviews_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWXKxz2Ylqrn"
      },
      "source": [
        "# Text Preprocessing:\n",
        "\n",
        "# For Text Preprocessing we will use TextBlob Library.TextBlob is built upon NLTK and provides an easy to use interface to the NLTK library.\n",
        "# Stopwords removal does not yield better results because VADER uses words such as 'but' in calculating the compound score.\n",
        "# Therefore, we won't be removing stopwords from the text!\n",
        "\n",
        "# Lower casing:\n",
        "df['content'] = df['content'].str.lower()\n",
        "# Removing punctuations:\n",
        "df['content'] = df['content'].str.lower()\n",
        "# Lemmatization:\n",
        "df['content'] = df['content'].apply(lambda x: \" \".join([Word(word).\n",
        "lemmatize() for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nh5HBK-mCGO"
      },
      "source": [
        "# Function for getting the sentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62fQsEzSmdTo"
      },
      "source": [
        "# Generating sentiment for all the sentence present in the dataset:\n",
        "sentiment_comp=[]\n",
        "for row in result_df['content']:\n",
        "    \n",
        "    vs=analyzer.polarity_scores(row)\n",
        "    sentiment_comp.append(vs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-9wN6xnmdKi"
      },
      "source": [
        "# Creating new dataframe with sentiments\n",
        "df_sentiments=pd.DataFrame(sentiment_comp)\n",
        "# df_sentiments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84WixvxhmdFI"
      },
      "source": [
        "# Merging the sentiments back to reviews dataframe\n",
        "result_df = pd.concat([result_df.reset_index(drop=True), df_sentiments], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI57RLm5mpMM"
      },
      "source": [
        "# Finding percentage of positivity/negativity and adding a column to result_df\n",
        "percentage_comp = []\n",
        "for i in result_df['compound']:\n",
        "    if i > 0:\n",
        "        percentage = i*100\n",
        "    elif i < 0:\n",
        "        percentage = -i*100\n",
        "    else:\n",
        "        percentage = \" \"\n",
        "    percentage_comp.append(percentage)\n",
        "result_df['percentage'] = percentage_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjRECDKjmpH1"
      },
      "source": [
        "# Convert scores into positive, negetive and not defined sentiments using some threshold\n",
        "result_df[\"Sentiment\"] = result_df[\"compound\"].apply(lambda compound: \"positive\" if compound > 0 else \\\n",
        "                                              (\"negative\" if compound < 0 else \"not defined\"))\n",
        "result_df.drop(['neg','neu','pos'], axis='columns', inplace=True)\n",
        "result_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nyGFuXXmpBi"
      },
      "source": [
        "# Convert scores into positive, negetive and not defined sentiments using some threshold\n",
        "result_df[\"Sentiment\"] = result_df[\"compound\"].apply(lambda compound: \"positive\" if compound > 0 else \\\n",
        "                                              (\"negative\" if compound < 0 else \"not defined\"))\n",
        "result_df.drop(['neg','neu','pos'], axis='columns', inplace=True)\n",
        "# result_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uv_rwiimdDU"
      },
      "source": [
        "result_df.to_csv(\"reviews_with_sentiments.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Vl82XM-I1D"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0wr4jQAsqcJ"
      },
      "source": [
        "In any machine learning task, cleaning or preprocessing the data is as important as model building. And when it comes to unstructured data like text, this process is even more important.\n",
        "\n",
        "Some of the common text preprocessing / cleaning steps I have done are:\n",
        "\n",
        "- Lower casing\n",
        "- Removal of Punctuations\n",
        "- Removal of Stopwords\n",
        "- Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMaVw1OSk5tF"
      },
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1fGkUNsmj8"
      },
      "source": [
        "# load the scraped data\n",
        "sample=pd.read_csv(\"reviews_with_sentiments.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgzrV_8zt4nI"
      },
      "source": [
        "## Lower Casing\n",
        "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n",
        "\n",
        "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnbEzK9LsnFz"
      },
      "source": [
        "sample[\"cleaned_text\"] = sample[\"content\"].str.lower()\n",
        "# sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGo9kxRuZfd"
      },
      "source": [
        "## Removal of Punctuations\n",
        "One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
        "\n",
        "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n",
        "\n",
        "!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
        "\n",
        "We can add or remove more punctuations as per our need.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxJfTWFbsowg"
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "sample[\"cleaned_text\"] = sample[\"cleaned_text\"].apply(lambda text: remove_punctuation(text))\n",
        "# sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHq6TWt0uuhK"
      },
      "source": [
        "## Removal of stopwords\n",
        "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis.\n",
        "\n",
        "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5opcZ1ELsot5"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URcvhrgZsooC"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "# exclude the words which you do not want to remove\n",
        "exclude_words = set((\"not\", \"no\"))\n",
        "new_stop_words = STOPWORDS.difference(exclude_words)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # custom function to remove the stopwords\n",
        "    return \" \".join([word for word in str(text).split() if word not in new_stop_words])\n",
        "\n",
        "sample[\"cleaned_text\"] = sample[\"cleaned_text\"].apply(lambda text: remove_stopwords(text))\n",
        "# sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulOOlBjCv6wf"
      },
      "source": [
        "## Stemming\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From Wikipedia)\n",
        "\n",
        "For example, if there are two words in the corpus 'walks' and 'walking', then stemming will stem the suffix to make them walk. But say in another example, we have two words 'console' and 'consoling', the stemmer will remove the suffix and make them consol which is not a proper english word.\n",
        "\n",
        "There are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLWagGwqsofw"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "sample[\"cleaned_text\"] = sample[\"cleaned_text\"].apply(lambda text: stem_words(text))\n",
        "# sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupVPhDF_W-c"
      },
      "source": [
        "# Text Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzKraHB_3W9"
      },
      "source": [
        "Text clustering is done using tf-idf and k-means clustering and then the final clusteres are formed using n-grams method. In this n-grams technique, you have to make n-grams for the most common words in each cluster and pick only those words which belong to that particular category, basically, the words which are more frequent in that cluster.\n",
        "\n",
        "Any other method can be used for text clustering depending on different datasets and any desired number of categories can be formed for that dataset.\n",
        "\n",
        "Here, seven categories were made after clustering and then stored into a csv called 'clustered_data.csv' which is provided in the repository for direct use. It shows the format in which the clustered data was made by doing some data wrangling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKrRfRezxZYm"
      },
      "source": [
        "## K means using TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGBC_N-bxOHF"
      },
      "source": [
        "#tfidf vector initililization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf = tfidf_vect.fit_transform(sample['cleaned_text'].values.astype('U'))\n",
        "# tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMKEnsvLxN_v"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model_tf = KMeans(n_clusters = 7, n_jobs = -1,random_state=99)\n",
        "model_tf.fit(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWUTjhARxN5a"
      },
      "source": [
        "# to understand what kind of words are generated\n",
        "terms1 = tfidf_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvO6IIRmyUzj"
      },
      "source": [
        "# Giving Labels/assigning a cluster to each text \n",
        "df1 = sample\n",
        "df1['Tfidf Clus Label'] = model_tf.labels_\n",
        "df1.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuVlx2PSyUtv"
      },
      "source": [
        "# How many points belong to each cluster \n",
        "df1.groupby(['Tfidf Clus Label'])['content'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t0KcjjryUmn"
      },
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(7):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms1[ind], end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB-_NlMyzLya"
      },
      "source": [
        "# how reviews are distributed across 7 clusters \n",
        "from matplotlib import pyplot as plt\n",
        "plt.bar([x for x in range(7)], df1.groupby(['Tfidf Clus Label'])['content'].count(), alpha = 0.4)\n",
        "plt.title('KMeans cluster points')\n",
        "plt.xlabel(\"Cluster number\")\n",
        "plt.ylabel(\"Number of points\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERJJdW4LzLu4"
      },
      "source": [
        "# Reading a review which belong to each group.\n",
        "for i in range(7):\n",
        "    print(\"5 reviews assigned to cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][0]]['content'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][1]]['content'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][2]]['content'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][3]]['content'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][4]]['content'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][5]]['content'])\n",
        "    print('\\n')\n",
        "    \n",
        "    print(\"_\" * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCUNkgSM0GUH"
      },
      "source": [
        "After making the clusters using TF-IDF and K-means you will get an idea about what are the most common topics/problems which people have spoken about in the reviews. Now, I formed n-grams for the most common and frequent topics in each cluster and formed new cluster of words(my own dictionary of words).\n",
        "\n",
        "First, seperated the negative reviews and performed this n-grams clustering technique only on the negative reviews and form the dataset in one-hot encoding format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CPlGSbfzLou"
      },
      "source": [
        "# seperating the negative reviews\n",
        "sample = sample[sample['Sentiment'] == 'negative']\n",
        "len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCo2l_WxNyS"
      },
      "source": [
        "# filtering the required columns\n",
        "sample = sample[['reviewId','content','CleanedText']]\n",
        "#sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8qb7OmX-I04"
      },
      "source": [
        "# cluster 1 contains problems with recharge\n",
        "cluster1 = ['recharg','reacharg','richarg','racharg'] \n",
        "df1 = pd.DataFrame()\n",
        "\n",
        "for word in cluster1:\n",
        "    cluster_1_reviews = []\n",
        "    for text in sample['CleanedText']:\n",
        "        if text.count(word)>0:\n",
        "            cluster_1_reviews.append(\"1\")\n",
        "        else:\n",
        "            cluster_1_reviews.append(\"0\")\n",
        "    sample['Problem in recharge'] = cluster_1_reviews\n",
        "    temp_df = sample[sample['Problem in recharge'] == \"1\"]\n",
        "    df1 = pd.concat([df1, temp_df], axis=0)\n",
        "    sample = sample[sample['Problem in recharge'] == \"0\"]\n",
        "# len(df1)\n",
        "sample = pd.concat([df1, sample], axis=0)\n",
        "# len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GPbxdY0-qyG"
      },
      "source": [
        "# check the number of reviews in that category\n",
        "sample['Problem in recharge'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGAAnQca-qe2"
      },
      "source": [
        "cluster2 = ['point','redeem','reedem','reward','redemp','redem'] \n",
        "df2 = pd.DataFrame()\n",
        "\n",
        "for word in cluster2:\n",
        "    cluster_2_reviews = []\n",
        "    for text in sample['CleanedText']:\n",
        "        if text.count(word)>0:\n",
        "            cluster_2_reviews.append(\"1\")\n",
        "        else:\n",
        "            cluster_2_reviews.append(\"0\")\n",
        "    sample['Problem in reward/redeem points'] = cluster_2_reviews\n",
        "    temp_df = sample[sample['Problem in reward/redeem points'] == \"1\"]\n",
        "    df2 = pd.concat([df2, temp_df], axis=0)\n",
        "    sample = sample[sample['Problem in reward/redeem points'] == \"0\"]\n",
        "# len(df2)\n",
        "sample = pd.concat([df2, sample], axis=0)\n",
        "# len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVUjlLgP_pkl"
      },
      "source": [
        "sample['Problem in reward/redeem points'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mpeQz8L_rul"
      },
      "source": [
        "cluster3 = ['regist','ragist','login','sign','signin','userid','usernam','user name','user id','pissword','passward','log','password','invalid user','user invalid','incorrect user','user incorrect','wrong user','user wrong','user work','enter correct user','say invalid user','password user passwird','user passwird dont','passwird dont know','user passwird dont','ragist','tri login','login say','pleas enter valid','enter correct user','abl sign','app sign','unabl sign','time sign','problem sign','sign say','error sign','unabl signup','unabl signup','sign sign','app unabl signup','poor troubl sign','sign bank employ','app abl sign','abl sign say','sign say sorri','abl sign worst','sign worst app','occur error sign','thrice unabl sign','unabl sign error','sign error messag','signup'] \n",
        "df3 = pd.DataFrame()\n",
        "\n",
        "for word in cluster3:\n",
        "    cluster_3_reviews = []\n",
        "    for text in sample['CleanedText']:\n",
        "        if text.count(word)>0:\n",
        "            cluster_3_reviews.append(\"1\")\n",
        "        else:\n",
        "            cluster_3_reviews.append(\"0\")\n",
        "    sample['Problem in registration/login/username/password'] = cluster_3_reviews\n",
        "    temp_df = sample[sample['Problem in registration/login/username/password'] == \"1\"]\n",
        "    df3 = pd.concat([df3, temp_df], axis=0)\n",
        "    sample = sample[sample['Problem in registration/login/username/password'] == \"0\"]\n",
        "# len(df3)\n",
        "sample = pd.concat([df3, sample], axis=0)\n",
        "# len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryT6cmZ__roV"
      },
      "source": [
        "sample['Problem in registration/login/username/password'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8jiSRsAJJJ"
      },
      "source": [
        "cluster4 = ['survic','costum','custom','servic','repli mail','didnt pick','pick call','mail repli','respons','respond','doesnot responss','responss','responc','poor servic','bad servic','custom care','custom id','worst servic','custom servic','custom support','custom care servic','worst custom servic','custom care number'] \n",
        "df4 = pd.DataFrame()\n",
        "\n",
        "for word in cluster4:\n",
        "    cluster_4_reviews = []\n",
        "    for text in sample['CleanedText']:\n",
        "        if text.count(word)>0:\n",
        "            cluster_4_reviews.append(\"1\")\n",
        "        else:\n",
        "            cluster_4_reviews.append(\"0\")\n",
        "    sample['Problem with customer care service'] = cluster_4_reviews\n",
        "    temp_df = sample[sample['Problem with customer care service'] == \"1\"]\n",
        "    df4 = pd.concat([df4, temp_df], axis=0)\n",
        "    sample = sample[sample['Problem with customer care service'] == \"0\"]\n",
        "# len(df4)\n",
        "sample = pd.concat([df4, sample], axis=0)\n",
        "# len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvE0ZBTKAI_T"
      },
      "source": [
        "sample['Problem with customer care service'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um4ustYTAI0-"
      },
      "source": [
        "cluster5 = ['app open','app work','work proper','open app','someth went','went wrong','alway error','crash','txn fail','complet txn','devic root','never update','time updat','problem updat','doesnt work','transact fail','user friend','open time','otp verif','verif fail','long time','work time','card number','time open','alway hang','payment fail','doesnt open','pathet slow','mobil number','bad work','doe open','product sbi','error someth','app crash','fail everi','app slow','abl pay','gift card','tri later','time error','fail tri','doe work','fail time','open pleas','app time','slow work','work app','sever time','server error','app error','latest version','open account','unabl process','app updat','app instal','noth work','app oppen','otp','server problem','error card','card invalid','fail transact','instal fail','couldnt open','worst updat','never detect','detect card','load','bug','connect error','unabl open','complaint resolv','tranction go slow','new version','never work','couldnt creat account','creat account','open account','failur transect','server','add product','limit product','tri instal','last updat','updat work','keep load','never open','even updat','problem connect','frequent stop','stop frequent','sorri problem','dont work','function proper','show error','error app','network oper','network didnt','otp','error transact','product','worst softwar','softwar','transact alway fail','amount deduct','date birth','order','orderd','courier','facil deliveri','deliveri','app fail','movi','updat contact','contact updat','updat poor','poor updat','cashback offer','defect item','item deliv','transact get fail','network error','technic error','much delay','transact updat','payment gateway','add contact','profil check','chang option','option avail','price seem cost','price cost','cost price','grievanc redress','redress system','order cancel','updat last month','book ticket','ticket book','email edit','option availbl','mobil get updat','mobil updat','android','updat data','transact updat','updat delay','payment refund','transact histori','bought book','bought','error open','limit shop','limit brand','interfac problem','shown error','internet bank','proper updat','surver issu','error occur','sometim error','due error','enter dob','merchant','problem app','seem problem','cant open','error massag','transact got failur','time transact','broken','sometim error','error arriv','chang email','chang mobil','problem occur','fail mani time','card chang','amount deduct sucess','limit option','order accept','buy','voucher','stop work','difficult work','updat app','updat useless','sudden stop','time work','suck updat','updat bad','bad updat','new updat','latest updat','updat issu','useless updat','updat version','updat disgust','disgust updat','close everytim','network select','experi updat','updat app','slow rune','poor updat','cant mainten','mainten proper','ofter updat','after updat','befor updat','fail updat','new updat','updat suck','suck updat','troubl open','offer avail','recent updat','lot issu','much lag','hell updat','everi updat','tym open','current updat','show tri','error occur','updat transact','transact miss','merchandis','gift voucher','cancel order','jio','keep stop','problem instal'] \n",
        "df5 = pd.DataFrame()\n",
        "\n",
        "for word in cluster5:\n",
        "    cluster_5_reviews = []\n",
        "    for text in sample['CleanedText']:\n",
        "        if text.count(word)>0:\n",
        "            cluster_5_reviews.append(\"1\")\n",
        "        else:\n",
        "            cluster_5_reviews.append(\"0\")\n",
        "    sample['Other complaints'] = cluster_5_reviews\n",
        "    temp_df = sample[sample['Other complaints'] == \"1\"]\n",
        "    df5 = pd.concat([df5, temp_df], axis=0)\n",
        "    sample = sample[sample['Other complaints'] == \"0\"]\n",
        "# len(df5)\n",
        "sample = pd.concat([df5, sample], axis=0)\n",
        "# len(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PEpERDcBFft"
      },
      "source": [
        "Do this similarly for positive and neutral reviews as well and then concatenate them together to form a single dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCr9BWhqAbTD"
      },
      "source": [
        "# df.to_csv('clustered_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p6amibrBjZf"
      },
      "source": [
        "# Building the Classifier Model\n",
        "A pretrained **XLNet model** has been used for classification and then this pretrained XLNet model was fine tuned by training it on the clustered data. And, after preprocessing the data and training the model on this data, a function was formed for getting predictions for a new data along with the probabilities of each label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwTHAUqXk7_B"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
        "from transformers import XLNetTokenizerFast\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZbLvzAKlmsP"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Xf8DAOl0XH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "df9aa02b-6ce5-495a-bafb-00d8ae6ef349"
      },
      "source": [
        "df = pd.read_csv('clustered_data.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewId</th>\n",
              "      <th>content</th>\n",
              "      <th>Problem in recharge</th>\n",
              "      <th>Problem in reward/redeem points</th>\n",
              "      <th>Problem in registration/login/username/password</th>\n",
              "      <th>Problem with customer care service</th>\n",
              "      <th>Other complaints</th>\n",
              "      <th>Bad comments</th>\n",
              "      <th>Appreciation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gp:AOqpTOGke5oHGTtgbNCNSurTU8c4h9j0aJgkGSKqvlO...</td>\n",
              "      <td>for mobile recharge, this is very excellent app.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gp:AOqpTOFGUaCTHYLrzFzaAusdm-wc1Rm9cUyHjgzzuwY...</td>\n",
              "      <td>awesome app for recharge and collect point any...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gp:AOqpTOFldCmXCDu4Ji5GGBZheX1k057zvjenXeydSCg...</td>\n",
              "      <td>smooth and trouble free recharge.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gp:AOqpTOHZne9EjZEQd8AWRd4rKr-Jmzk_nCQPb2wOqJp...</td>\n",
              "      <td>very nice app for revard and to use it to rech...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gp:AOqpTOHlQooz_z4BGtqsPAlx99zdOvJOt6Nj3OBCkEA...</td>\n",
              "      <td>good for mobile recharge</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            reviewId  ... Appreciation\n",
              "0  gp:AOqpTOGke5oHGTtgbNCNSurTU8c4h9j0aJgkGSKqvlO...  ...            1\n",
              "1  gp:AOqpTOFGUaCTHYLrzFzaAusdm-wc1Rm9cUyHjgzzuwY...  ...            1\n",
              "2  gp:AOqpTOFldCmXCDu4Ji5GGBZheX1k057zvjenXeydSCg...  ...            1\n",
              "3  gp:AOqpTOHZne9EjZEQd8AWRd4rKr-Jmzk_nCQPb2wOqJp...  ...            1\n",
              "4  gp:AOqpTOHlQooz_z4BGtqsPAlx99zdOvJOt6Nj3OBCkEA...  ...            1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTwkEX4JCziK",
        "outputId": "f8c0c235-d28b-4081-96d7-b8d331b5c07c"
      },
      "source": [
        "# Split the data:\n",
        "train, test = train_test_split(df, test_size=0.05)\n",
        "train.shape, test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14747, 9), (777, 9))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1CjLi8LdP1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "eb0f5ec2-0f01-447a-93f7-45db39038e09"
      },
      "source": [
        "train = train.set_index('reviewId')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>Problem in recharge</th>\n",
              "      <th>Problem in reward/redeem points</th>\n",
              "      <th>Problem in registration/login/username/password</th>\n",
              "      <th>Problem with customer care service</th>\n",
              "      <th>Other complaints</th>\n",
              "      <th>Bad comments</th>\n",
              "      <th>Appreciation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOH8jCaAwdk9-qE_MmxJ97tAXE30k37mUBiYpR7cmyyDyw0HMuUrD-URMKP3q-9kr3LXDshnFqO-hgDFRyA</th>\n",
              "      <td>waste of using this app , could not activate m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOEjgWcte_Y0Afg-qIeTJ-rUJnc_r__g3_6OjfGBqcGCT-1xrVtGzQV7YnxxsY8gel4ZJvGAKR3buKSrCU0</th>\n",
              "      <td>good</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGniFIBHNn4xutfeb3zYgfS1_nomysuhv6iXaa0LLb_qq9bGJdWUQrn4J1xlSJY050ED2c9GirVxiOSROE</th>\n",
              "      <td>not good</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGQoTMiP_dpiv7tnjkvRfkqQDeR5_Ffcz7vqNMlz59LaZ0AJjltwYf8TfFnBvW2iAutxdozt7F8u3Va7UU</th>\n",
              "      <td>mobile recharge is not done while reward point...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHJsvpGEYcklDS7kYRCSjFeO7dWTCRLSvFHxOVsUUgcvzanxIvIpdRJGEsKw8axauDb_2y2s8BpnhiziR0</th>\n",
              "      <td>worst experience, terrible performance by the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              content  ...  Appreciation\n",
              "reviewId                                                                                               ...              \n",
              "gp:AOqpTOH8jCaAwdk9-qE_MmxJ97tAXE30k37mUBiYpR7c...  waste of using this app , could not activate m...  ...             0\n",
              "gp:AOqpTOEjgWcte_Y0Afg-qIeTJ-rUJnc_r__g3_6OjfGB...                                               good  ...             1\n",
              "gp:AOqpTOGniFIBHNn4xutfeb3zYgfS1_nomysuhv6iXaa0...                                           not good  ...             0\n",
              "gp:AOqpTOGQoTMiP_dpiv7tnjkvRfkqQDeR5_Ffcz7vqNMl...  mobile recharge is not done while reward point...  ...             0\n",
              "gp:AOqpTOHJsvpGEYcklDS7kYRCSjFeO7dWTCRLSvFHxOVs...  worst experience, terrible performance by the ...  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWX6qJg-dRcR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "110ae795-f093-4896-b6e0-f426c5fb7b18"
      },
      "source": [
        "test = test[['reviewId','content']]\n",
        "test = test.set_index('reviewId')\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewId</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4AEjJiPK90jg-o56Vf6X_Mtfbcibv6dEw3b5IqXR5mz0</th>\n",
              "      <td>good boy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZxnx8wgNl6oecPTvUEDZBJ1ix3ovWeYvsMgNUlNfdMJc</th>\n",
              "      <td>very nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXhLDyl6RZHpKyuO15V_kambTEUfczaPvSeQKTdYwfwVNI</th>\n",
              "      <td>bad app</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby9COr_vTk1c6Axe-T2jeejxwKsEaAHbbs7onwe3EQLPU</th>\n",
              "      <td>worst app</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg9exW6eF5BA-b50g5XRXnVJBlbuyTSNhoOyonPZ-4Yzo</th>\n",
              "      <td>i have successfully registered but when i try ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              content\n",
              "reviewId                                                                                             \n",
              "gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4...                                           good boy\n",
              "gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZ...                                          very nice\n",
              "gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXh...                                            bad app\n",
              "gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby...                                          worst app\n",
              "gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg...  i have successfully registered but when i try ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz9OzE2TmSN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d03d78-2d1b-4e2d-fe04-d8aeaec38108"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14747, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW3dJFOTMkq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63916d5-7e80-4f7c-aa97-ed7f11928677"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(777, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN8bQOt2mVc5"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTOd-qKQMyFO"
      },
      "source": [
        "# tokenize data\n",
        "tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Xq-qP54SiF"
      },
      "source": [
        "train_text_list = train[\"content\"].values\n",
        "test_text_list = test[\"content\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocveMTIfM5cP"
      },
      "source": [
        "def tokenize_inputs(text_list, tokenizer, num_embeddings=512):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text input into ids. Appends the appropriate special\n",
        "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
        "    the appropriate sequence length.\n",
        "    \"\"\"\n",
        "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
        "    # the 2 special characters\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
        "    # convert tokenized text into numeric ids for the appropriate LM\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    # append special token \"<s>\" and </s> to end of sentence\n",
        "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
        "    # pad sequences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    return input_ids\n",
        "\n",
        "def create_attn_masks(input_ids):\n",
        "    \"\"\"\n",
        "    Create attention masks to tell model whether attention should be applied to\n",
        "    the input id tokens. Do not want to perform attention on padding tokens.\n",
        "    \"\"\"\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZVLj_OM8sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d13eda6-19e4-49dd-8178-2a7adfd70481"
      },
      "source": [
        "# create input id tokens\n",
        "train_input_ids = tokenize_inputs(train_text_list, tokenizer, num_embeddings=250)\n",
        "train_input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3419,   20,  381, ...,    0,    0,    0],\n",
              "       [ 195,    4,    3, ...,    0,    0,    0],\n",
              "       [  50,  195,    4, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  36,   26,   23, ...,    0,    0,    0],\n",
              "       [  36,  172, 2101, ...,    0,    0,    0],\n",
              "       [  50,  195,    4, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrzAuQ_6M-ER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106d0190-74d7-4c32-ef3f-b712cc0d76c3"
      },
      "source": [
        "# create input id tokens\n",
        "test_input_ids = tokenize_inputs(test_text_list, tokenizer, num_embeddings=250)\n",
        "test_input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 195, 2001,    4, ...,    0,    0,    0],\n",
              "       [ 172, 2101,    4, ...,    0,    0,    0],\n",
              "       [ 948, 5523,    4, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  17,  150,   17, ...,    0,    0,    0],\n",
              "       [ 312, 5523,   21, ...,    0,    0,    0],\n",
              "       [ 195,    4,    3, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgGLmaKKM_xB"
      },
      "source": [
        "# create attention masks\n",
        "train_attention_masks = create_attn_masks(train_input_ids)\n",
        "# train_attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdX4QlnZTeaN"
      },
      "source": [
        "# create attention masks\n",
        "test_attention_masks = create_attn_masks(test_input_ids)\n",
        "# test_attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQPuY4mONCSu"
      },
      "source": [
        "# add input ids and attention masks to the dataframe\n",
        "train[\"features\"] = train_input_ids.tolist()\n",
        "train[\"masks\"] = train_attention_masks\n",
        "\n",
        "test[\"features\"] = test_input_ids.tolist()\n",
        "test[\"masks\"] = test_attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjzFlRljNEkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6031c9c6-a29d-412b-9568-14572637eb51"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>Problem in recharge</th>\n",
              "      <th>Problem in reward/redeem points</th>\n",
              "      <th>Problem in registration/login/username/password</th>\n",
              "      <th>Problem with customer care service</th>\n",
              "      <th>Other complaints</th>\n",
              "      <th>Bad comments</th>\n",
              "      <th>Appreciation</th>\n",
              "      <th>features</th>\n",
              "      <th>masks</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOH8jCaAwdk9-qE_MmxJ97tAXE30k37mUBiYpR7cmyyDyw0HMuUrD-URMKP3q-9kr3LXDshnFqO-hgDFRyA</th>\n",
              "      <td>waste of using this app , could not activate m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[3419, 20, 381, 52, 5523, 17, 19, 121, 50, 177...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOEjgWcte_Y0Afg-qIeTJ-rUJnc_r__g3_6OjfGBqcGCT-1xrVtGzQV7YnxxsY8gel4ZJvGAKR3buKSrCU0</th>\n",
              "      <td>good</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[195, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGniFIBHNn4xutfeb3zYgfS1_nomysuhv6iXaa0LLb_qq9bGJdWUQrn4J1xlSJY050ED2c9GirVxiOSROE</th>\n",
              "      <td>not good</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[50, 195, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGQoTMiP_dpiv7tnjkvRfkqQDeR5_Ffcz7vqNMlz59LaZ0AJjltwYf8TfFnBvW2iAutxdozt7F8u3Va7UU</th>\n",
              "      <td>mobile recharge is not done while reward point...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[2487, 23140, 27, 50, 588, 171, 8614, 424, 186...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHJsvpGEYcklDS7kYRCSjFeO7dWTCRLSvFHxOVsUUgcvzanxIvIpdRJGEsKw8axauDb_2y2s8BpnhiziR0</th>\n",
              "      <td>worst experience, terrible performance by the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[2598, 656, 19, 6518, 922, 37, 18, 5523, 9, 4,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              content  ...                                              masks\n",
              "reviewId                                                                                               ...                                                   \n",
              "gp:AOqpTOH8jCaAwdk9-qE_MmxJ97tAXE30k37mUBiYpR7c...  waste of using this app , could not activate m...  ...  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
              "gp:AOqpTOEjgWcte_Y0Afg-qIeTJ-rUJnc_r__g3_6OjfGB...                                               good  ...  [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOGniFIBHNn4xutfeb3zYgfS1_nomysuhv6iXaa0...                                           not good  ...  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOGQoTMiP_dpiv7tnjkvRfkqQDeR5_Ffcz7vqNMl...  mobile recharge is not done while reward point...  ...  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
              "gp:AOqpTOHJsvpGEYcklDS7kYRCSjFeO7dWTCRLSvFHxOVs...  worst experience, terrible performance by the ...  ...  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcrK191NG_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "52932156-8ad7-473c-82a2-cd23dde4b668"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>features</th>\n",
              "      <th>masks</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4AEjJiPK90jg-o56Vf6X_Mtfbcibv6dEw3b5IqXR5mz0</th>\n",
              "      <td>good boy</td>\n",
              "      <td>[195, 2001, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZxnx8wgNl6oecPTvUEDZBJ1ix3ovWeYvsMgNUlNfdMJc</th>\n",
              "      <td>very nice</td>\n",
              "      <td>[172, 2101, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXhLDyl6RZHpKyuO15V_kambTEUfczaPvSeQKTdYwfwVNI</th>\n",
              "      <td>bad app</td>\n",
              "      <td>[948, 5523, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby9COr_vTk1c6Axe-T2jeejxwKsEaAHbbs7onwe3EQLPU</th>\n",
              "      <td>worst app</td>\n",
              "      <td>[2598, 5523, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg9exW6eF5BA-b50g5XRXnVJBlbuyTSNhoOyonPZ-4Yzo</th>\n",
              "      <td>i have successfully registered but when i try ...</td>\n",
              "      <td>[17, 150, 47, 3918, 2815, 57, 90, 17, 150, 714...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              content  ...                                              masks\n",
              "reviewId                                                                                               ...                                                   \n",
              "gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4...                                           good boy  ...  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZ...                                          very nice  ...  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXh...                                            bad app  ...  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby...                                          worst app  ...  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg...  i have successfully registered but when i try ...  ...  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRpKDj-WN2tN"
      },
      "source": [
        "# Train, Valid Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1SXXP4GN1m_"
      },
      "source": [
        "# split into train and valid\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2A1pqzcN_Gr"
      },
      "source": [
        "X_train = train[\"features\"].values.tolist()\n",
        "X_valid = valid[\"features\"].values.tolist()\n",
        "\n",
        "train_masks = train[\"masks\"].values.tolist()\n",
        "valid_masks = valid[\"masks\"].values.tolist()\n",
        "\n",
        "label_cols = ['Problem in recharge','Problem in reward/redeem points','Problem in registration/login/username/password','Problem with customer care service','Other complaints','Bad comments','Appreciation']\n",
        "Y_train = train[label_cols].values.tolist()\n",
        "Y_valid = valid[label_cols].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQUZkER-OCVE"
      },
      "source": [
        "# Create Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jChp-AsjOBrv"
      },
      "source": [
        "# create dataloaders\n",
        "# Convert all of our input ids and attention masks into \n",
        "# torch tensors, the required datatype for our model\n",
        "\n",
        "X_train = torch.tensor(X_train)\n",
        "X_valid = torch.tensor(X_valid)\n",
        "\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "Y_valid = torch.tensor(Y_valid, dtype=torch.float32)\n",
        "\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "valid_masks = torch.tensor(valid_masks, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMeaH6gbOHM1"
      },
      "source": [
        "# Batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader.\n",
        "train_data = TensorDataset(X_train, train_masks, Y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,\\\n",
        "                              sampler=train_sampler,\\\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(X_valid, valid_masks, Y_valid)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,\\\n",
        "                                   sampler=validation_sampler,\\\n",
        "                                   batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCE_3AnLOPpI"
      },
      "source": [
        "def train(model, num_epochs,\\\n",
        "          optimizer,\\\n",
        "          train_dataloader, valid_dataloader,\\\n",
        "          model_save_path,\\\n",
        "          train_loss_set=[], valid_loss_set = [],\\\n",
        "          lowest_eval_loss=None, start_epoch=0,\\\n",
        "          device=\"cpu\"\n",
        "          ):\n",
        "  \"\"\"\n",
        "  Train the model and save the model with the lowest validation loss\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  # trange is a tqdm wrapper around the normal python range\n",
        "  for i in trange(num_epochs, desc=\"Epoch\"):\n",
        "    # if continue training from saved model\n",
        "    actual_epoch = start_epoch + i\n",
        "\n",
        "    # Training\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Clear out the gradients (by default they accumulate)\n",
        "      optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "      # store train loss\n",
        "      tr_loss += loss.item()\n",
        "      num_train_samples += b_labels.size(0)\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      #scheduler.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    epoch_train_loss = tr_loss/num_train_samples\n",
        "    train_loss_set.append(epoch_train_loss)\n",
        "\n",
        "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the validation set\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss = 0\n",
        "    num_eval_samples = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Telling the model not to compute or store gradients,\n",
        "      # saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate validation loss\n",
        "        loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # store valid loss\n",
        "        eval_loss += loss.item()\n",
        "        num_eval_samples += b_labels.size(0)\n",
        "\n",
        "    epoch_eval_loss = eval_loss/num_eval_samples\n",
        "    valid_loss_set.append(epoch_eval_loss)\n",
        "\n",
        "    print(\"Valid loss: {}\".format(epoch_eval_loss))\n",
        "\n",
        "    if lowest_eval_loss == None:\n",
        "      lowest_eval_loss = epoch_eval_loss\n",
        "      # save model\n",
        "      save_model(model, model_save_path, actual_epoch,\\\n",
        "                 lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "    else:\n",
        "      if epoch_eval_loss < lowest_eval_loss:\n",
        "        lowest_eval_loss = epoch_eval_loss\n",
        "        # save model\n",
        "        save_model(model, model_save_path, actual_epoch,\\\n",
        "                   lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return model, train_loss_set, valid_loss_set\n",
        "\n",
        "\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n",
        "  \"\"\"\n",
        "  Save the model to the path directory provided\n",
        "  \"\"\"\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model\n",
        "  checkpoint = {'epochs': epochs, \\\n",
        "                'lowest_eval_loss': lowest_eval_loss,\\\n",
        "                'state_dict': model_to_save.state_dict(),\\\n",
        "                'train_loss_hist': train_loss_hist,\\\n",
        "                'valid_loss_hist': valid_loss_hist\n",
        "               }\n",
        "  torch.save(checkpoint, save_path)\n",
        "  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n",
        "                                                                     lowest_eval_loss))\n",
        "  return\n",
        "  \n",
        "def load_model(save_path):\n",
        "  \"\"\"\n",
        "  Load the model from the path directory provided\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(save_path)\n",
        "  model_state_dict = checkpoint['state_dict']\n",
        "  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
        "  model.load_state_dict(model_state_dict)\n",
        "\n",
        "  epochs = checkpoint[\"epochs\"]\n",
        "  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n",
        "  train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
        "  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n",
        "  \n",
        "  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoxSrbFDOUW0"
      },
      "source": [
        "# Train Model from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGMnhaIuUbXx"
      },
      "source": [
        "# train model from scratch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VMI7VWpOKj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e49c8c-471e-48c4-baf4-ce1af47a4da1"
      },
      "source": [
        "#config = XLNetConfig()\n",
        "        \n",
        "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, num_labels=2):\n",
        "    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "    self.classifier = torch.nn.Linear(768, num_labels)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids=None,\\\n",
        "              attention_mask=None, labels=None):\n",
        "    # last hidden layer\n",
        "    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
        "                                   attention_mask=attention_mask,\\\n",
        "                                   token_type_ids=token_type_ids)\n",
        "    # pool the outputs into a mean vector\n",
        "    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
        "    logits = self.classifier(mean_last_hidden_state)\n",
        "        \n",
        "    if labels is not None:\n",
        "      loss_fct = BCEWithLogitsLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels),\\\n",
        "                      labels.view(-1, self.num_labels))\n",
        "      return loss\n",
        "    else:\n",
        "      return logits\n",
        "    \n",
        "  def freeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Freeze XLNet weight parameters. They will not be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "  def unfreeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Unfreeze XLNet weight parameters. They will be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = True\n",
        "    \n",
        "  def pool_hidden_state(self, last_hidden_state):\n",
        "    \"\"\"\n",
        "    Pool the output vectors into a single mean vector \n",
        "    \"\"\"\n",
        "    last_hidden_state = last_hidden_state[0]\n",
        "    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
        "    return mean_last_hidden_state\n",
        "    \n",
        "model = XLNetForMultiLabelSequenceClassification(num_labels=len(Y_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QuSJxanOMRy"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTMnS064cWu4",
        "outputId": "ef90f41a-1eee-4db8-894f-3c2efaf6a402"
      },
      "source": [
        "# import drive in colab to get a model save path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxCN5nLvOO2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b99b5a-e891-460c-c1c9-58ea096d1feb"
      },
      "source": [
        "# training..\n",
        "num_epochs=3\n",
        "\n",
        "model_save_name = 'classifier_model1.pt'\n",
        "model_save_path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "model, train_loss_set, valid_loss_set = train(model=model,\\\n",
        "                                              num_epochs=num_epochs,\\\n",
        "                                              optimizer=optimizer,\\\n",
        "                                              train_dataloader=train_dataloader,\\\n",
        "                                              valid_dataloader=validation_dataloader,\\\n",
        "                                              model_save_path=model_save_path,\\\n",
        "                                              device=\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0054599517184351146\n",
            "Valid loss: 0.003236652824167415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [15:20<30:41, 920.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model at epoch 0 with validation loss of 0.003236652824167415\n",
            "\n",
            "\n",
            "Train loss: 0.0022549303461320367\n",
            "Valid loss: 0.0028260064589883736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [30:38<15:19, 919.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model at epoch 1 with validation loss of 0.0028260064589883736\n",
            "\n",
            "\n",
            "Train loss: 0.0014024759660813371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [45:54<00:00, 918.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Valid loss: 0.003301545290242618\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHt9MdUoOaLl"
      },
      "source": [
        "# Train Model From Previous Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzZEnoI3Ob2u"
      },
      "source": [
        "# train model from previous checkpoint\n",
        "model_save_name = 'classifier_model1.pt'\n",
        "model_save_path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "model, start_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist = load_model(model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrTdby8Uaqqb"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiwYWrPsREQ-"
      },
      "source": [
        "num_epochs=3\n",
        "model, train_loss_set, valid_loss_set = train(model=model,\\\n",
        "                                              num_epochs=num_epochs,\\\n",
        "                                              optimizer=optimizer,\\\n",
        "                                              train_dataloader=train_dataloader,\\\n",
        "                                              valid_dataloader=validation_dataloader,\\\n",
        "                                              model_save_path=model_save_path,\\\n",
        "                                              train_loss_set=train_loss_hist,\\\n",
        "                                              valid_loss_set=valid_loss_hist,\\\n",
        "                                              lowest_eval_loss=lowest_eval_loss,\\\n",
        "                                              start_epoch=start_epoch,\\\n",
        "                                              device=\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hfFhwOrRj4t"
      },
      "source": [
        "# save the model\n",
        "torch. save(model. state_dict(), model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PYfqX3QrE-"
      },
      "source": [
        "# Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey9FMYJxQqGs"
      },
      "source": [
        "# get predictions\n",
        "def generate_predictions(model, df, num_labels, device=\"cpu\", batch_size=32):\n",
        "  num_iter = math.ceil(df.shape[0]/batch_size)\n",
        "  \n",
        "  pred_probs = np.array([]).reshape(0, num_labels)\n",
        "  \n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  for i in range(num_iter):\n",
        "    df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n",
        "    X = df_subset[\"features\"].values.tolist()\n",
        "    masks = df_subset[\"masks\"].values.tolist()\n",
        "    X = torch.tensor(X)\n",
        "    masks = torch.tensor(masks, dtype=torch.long)\n",
        "    X = X.to(device)\n",
        "    masks = masks.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids=X, attention_mask=masks)\n",
        "      logits = logits.sigmoid().detach().cpu().numpy()\n",
        "      pred_probs = np.vstack([pred_probs, logits])\n",
        "  \n",
        "  return pred_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo0YyAH6XGTh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39fb3c8-b3b0-4ba1-95b6-faace04e2457"
      },
      "source": [
        "num_labels = len(label_cols)\n",
        "pred_probs = generate_predictions(model, test, num_labels, device=\"cuda\", batch_size=32)\n",
        "pred_probs = np.round(pred_probs, 3)\n",
        "# pred_probs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.26418785e-07, 7.21927762e-08, 5.00244113e-08, ...,\n",
              "        4.55935492e-07, 7.14609314e-08, 1.00000000e+00],\n",
              "       [3.10080992e-07, 6.26785166e-08, 5.39943521e-08, ...,\n",
              "        3.66178853e-07, 5.96945142e-08, 1.00000000e+00],\n",
              "       [5.12417091e-06, 8.35645210e-07, 1.46626360e-06, ...,\n",
              "        6.24060294e-06, 9.99872923e-01, 8.27477925e-05],\n",
              "       ...,\n",
              "       [1.15612900e-04, 8.55474151e-04, 1.63673230e-05, ...,\n",
              "        1.95052169e-04, 1.19782962e-05, 9.99762237e-01],\n",
              "       [1.29337241e-06, 5.01019429e-07, 1.80744493e-07, ...,\n",
              "        1.65683559e-06, 2.84675764e-07, 9.99999762e-01],\n",
              "       [2.73788032e-07, 6.32528128e-08, 4.51179254e-08, ...,\n",
              "        3.32330586e-07, 5.56356845e-08, 1.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVAWKam2ado5"
      },
      "source": [
        "label_cols = ['Problem in recharge','Problem in reward/redeem points','Problem in registration/login/username/password','Problem with customer care service','Other complaints','Bad comments','Appreciation']\n",
        "\n",
        "test['Problem in recharge'] = pred_probs[:,0]\n",
        "test['Problem in reward/redeem points'] = pred_probs[:,1]\n",
        "test['Problem in registration/login/username/password'] = pred_probs[:,2]\n",
        "test['Problem with customer care service'] = pred_probs[:,3]\n",
        "test['Other complaints'] = pred_probs[:,4]\n",
        "test['Bad comments'] = pred_probs[:,5]\n",
        "test['Appreciation'] = pred_probs[:,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_l6LVlX_R9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b76fba0b-2bed-40a1-9a10-8a23ffb74522"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>features</th>\n",
              "      <th>masks</th>\n",
              "      <th>Problem in recharge</th>\n",
              "      <th>Problem in reward/redeem points</th>\n",
              "      <th>Problem in registration/login/username/password</th>\n",
              "      <th>Problem with customer care service</th>\n",
              "      <th>Other complaints</th>\n",
              "      <th>Bad comments</th>\n",
              "      <th>Appreciation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4AEjJiPK90jg-o56Vf6X_Mtfbcibv6dEw3b5IqXR5mz0</th>\n",
              "      <td>good boy</td>\n",
              "      <td>[195, 2001, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>3.264188e-07</td>\n",
              "      <td>7.219278e-08</td>\n",
              "      <td>5.002441e-08</td>\n",
              "      <td>4.082840e-09</td>\n",
              "      <td>4.559355e-07</td>\n",
              "      <td>7.146093e-08</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZxnx8wgNl6oecPTvUEDZBJ1ix3ovWeYvsMgNUlNfdMJc</th>\n",
              "      <td>very nice</td>\n",
              "      <td>[172, 2101, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>3.100810e-07</td>\n",
              "      <td>6.267852e-08</td>\n",
              "      <td>5.399435e-08</td>\n",
              "      <td>4.083798e-09</td>\n",
              "      <td>3.661789e-07</td>\n",
              "      <td>5.969451e-08</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXhLDyl6RZHpKyuO15V_kambTEUfczaPvSeQKTdYwfwVNI</th>\n",
              "      <td>bad app</td>\n",
              "      <td>[948, 5523, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>5.124171e-06</td>\n",
              "      <td>8.356452e-07</td>\n",
              "      <td>1.466264e-06</td>\n",
              "      <td>1.589592e-07</td>\n",
              "      <td>6.240603e-06</td>\n",
              "      <td>9.998729e-01</td>\n",
              "      <td>0.000083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby9COr_vTk1c6Axe-T2jeejxwKsEaAHbbs7onwe3EQLPU</th>\n",
              "      <td>worst app</td>\n",
              "      <td>[2598, 5523, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>3.317411e-06</td>\n",
              "      <td>1.430770e-06</td>\n",
              "      <td>1.063038e-06</td>\n",
              "      <td>5.784522e-08</td>\n",
              "      <td>1.083658e-06</td>\n",
              "      <td>9.999573e-01</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg9exW6eF5BA-b50g5XRXnVJBlbuyTSNhoOyonPZ-4Yzo</th>\n",
              "      <td>i have successfully registered but when i try ...</td>\n",
              "      <td>[17, 150, 47, 3918, 2815, 57, 90, 17, 150, 714...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>1.100581e-03</td>\n",
              "      <td>9.970652e-01</td>\n",
              "      <td>9.960038e-01</td>\n",
              "      <td>3.026004e-04</td>\n",
              "      <td>8.967006e-01</td>\n",
              "      <td>1.224051e-04</td>\n",
              "      <td>0.000364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              content  ... Appreciation\n",
              "reviewId                                                                                               ...             \n",
              "gp:AOqpTOGTADrOKHFXYgD5jwLGK1Ult0gcnjTCj3fqrKk4...                                           good boy  ...     1.000000\n",
              "gp:AOqpTOHG20z1AXXSPa5cRQQXNS3Mri57rOo9JNu0MZBZ...                                          very nice  ...     1.000000\n",
              "gp:AOqpTOEjMUAO5sa_j77QG0hp75avoD2FwOjevIlNWcXh...                                            bad app  ...     0.000083\n",
              "gp:AOqpTOHxMcsJGLebsXniRuZk71y546Y-tCV0ME-gaZby...                                          worst app  ...     0.000028\n",
              "gp:AOqpTOFGo_lltI1X7gImeKrUe7tnKJT0Nst6IIujSWrg...  i have successfully registered but when i try ...  ...     0.000364\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqrwRwxFGOvK"
      },
      "source": [
        "# test.to_csv('xlnet_classifier.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AADartP_DxPg"
      },
      "source": [
        "# Get results for a single comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLp_WDgGvsl"
      },
      "source": [
        "comment = \"recharge was bad and the customer care did not respond and the app was not working\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx_ENYIS56vJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1ad969-6056-4db6-a7f4-fca9a0d69dff"
      },
      "source": [
        "data = [[comment]]\n",
        "df1 = pd.DataFrame(data, columns = ['content'])\n",
        "df1_text_list = df1[\"content\"].values\n",
        "df1_input_ids = tokenize_inputs(df1_text_list, tokenizer, num_embeddings=250)\n",
        "df1_attention_masks = create_attn_masks(df1_input_ids)\n",
        "df1[\"features\"] = df1_input_ids.tolist()\n",
        "df1[\"masks\"] = df1_attention_masks\n",
        "num_labels = len(label_cols)\n",
        "pred_probs = generate_predictions(model, df1, num_labels, device=\"cuda\", batch_size=1)\n",
        "pred_probs = np.round(pred_probs, 3)\n",
        "probsList = [ item for elem in pred_probs for item in elem]\n",
        "probsList        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.999, 0.001, 0.0, 1.0, 0.932, 0.0, 0.001]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcsFqGgW6VYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24768065-fc56-490d-e5bb-fa4098e28a4e"
      },
      "source": [
        "THRESHOLD = 0.5\n",
        "if probsList[0] < THRESHOLD and probsList[1] < THRESHOLD and probsList[2] < THRESHOLD and probsList[\n",
        "    3] < THRESHOLD and probsList[4] < THRESHOLD and probsList[5] < THRESHOLD and probsList[6] < THRESHOLD:\n",
        "    st.subheader(\"Your text does not belong to any category!\")\n",
        "else:\n",
        "    for label, prediction in zip(label_cols, probsList):\n",
        "        if prediction < THRESHOLD:\n",
        "            continue\n",
        "        print(f\"{label}: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem in recharge: 0.999\n",
            "Problem with customer care service: 1.0\n",
            "Other complaints: 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}